{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5c3202",
   "metadata": {},
   "source": [
    "## Model Compilation Jupyter Notebook Example\n",
    "\n",
    "This notebook shows the example of model compilation using edgeai-benchmark.\n",
    "\n",
    "This script uses TIDL to compile a model and output in a format that edgeai-sdk can understand.\n",
    "\n",
    "edgeai_benchmark is a python package provided in edgeai-benchmark that provides several functions to assist model compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fd730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import argparse\n",
    "import cv2\n",
    "import yaml\n",
    "import shutil\n",
    "from edgeai_benchmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f68d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIDL_TOOLS_PATH= /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/tools/AM62A/tidl_tools\n",
      "LD_LIBRARY_PATH= /home/a0393608local/.pyenv/versions/3.10.14/envs/benchmark/lib/python3.10/site-packages/cv2/../../lib64:/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/tools/AM62A/tidl_tools:\n",
      "TARGET_SOC= AM62A\n",
      "INFO: current dir is: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark\n"
     ]
    }
   ],
   "source": [
    "# the cwd must be the root of the respository\n",
    "if os.path.split(os.getcwd())[-1] in ('scripts', 'tutorials'):\n",
    "    os.chdir('../')\n",
    "#\n",
    "\n",
    "assert ('TIDL_TOOLS_PATH' in os.environ and 'LD_LIBRARY_PATH' in os.environ and\n",
    "        'TARGET_SOC' in os.environ), \"Check the environment variables\"\n",
    "print(\"TIDL_TOOLS_PATH=\", os.environ['TIDL_TOOLS_PATH'])\n",
    "print(\"LD_LIBRARY_PATH=\", os.environ['LD_LIBRARY_PATH'])\n",
    "print(\"TARGET_SOC=\", os.environ['TARGET_SOC'])\n",
    "\n",
    "print(f\"INFO: current dir is: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4bf6ec",
   "metadata": {},
   "source": [
    "#### Create a temporary directory. \n",
    "\n",
    "This is were the compiled artifacts will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed12c333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clearing modelartifacts: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts\n"
     ]
    }
   ],
   "source": [
    "modelartifacts_tempdir_name = os.path.abspath('./work_dirs_custom')\n",
    "modelartifacts_custom = os.path.join(modelartifacts_tempdir_name, 'modelartifacts')\n",
    "print(f'INFO: clearing modelartifacts: {modelartifacts_custom}')\n",
    "if os.path.exists(modelartifacts_custom):\n",
    "    shutil.rmtree(modelartifacts_custom)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acd99c",
   "metadata": {},
   "source": [
    "#### Read settings from settings_import_on_pc.yaml\n",
    "\n",
    "Modify the settings as necessary in the constructor of settings.ConfigSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b47491",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = config_settings.CustomConfigSettings('./settings_import_on_pc.yaml',\n",
    "                target_device = os.environ['TARGET_SOC'], target_machine = constants.TARGET_MACHINE_PC_EMULATION,\n",
    "                modelartifacts_path=modelartifacts_custom,\n",
    "                num_frames=100)\n",
    "\n",
    "work_dir = os.path.join(settings.modelartifacts_path, f'{settings.tensor_bits}bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937d8b1",
   "metadata": {},
   "source": [
    "#### Create Dataset Reader classes\n",
    "\n",
    "Change the dataset paths according to your dataset location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90745b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_calib_cfg = dict(\n",
    "    path=f'{settings.datasets_path}/imagenetv2c/val',\n",
    "    split=f'{settings.datasets_path}/imagenetv2c/val.txt',\n",
    "    num_classes=1000,\n",
    "    shuffle=True,\n",
    "    num_frames=min(settings.calibration_frames,50000),\n",
    "    name='imagenet'\n",
    ")\n",
    "\n",
    "# dataset parameters for actual inference\n",
    "dataset_val_cfg = dict(\n",
    "    path=f'{settings.datasets_path}/imagenetv2c/val',\n",
    "    split=f'{settings.datasets_path}/imagenetv2c/val.txt',\n",
    "    num_classes=1000,\n",
    "    shuffle=True,\n",
    "    num_frames=min(settings.num_frames,50000),\n",
    "    name='imagenet'\n",
    ")\n",
    "\n",
    "calib_dataset = datasets.ImageClassification(**dataset_calib_cfg)\n",
    "val_dataset = datasets.ImageClassification(**dataset_val_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb928333",
   "metadata": {},
   "source": [
    "#### Session runtime_options\n",
    "\n",
    "The default runtime_options can be overriden by passing a runtime_options dict to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0b4f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'edgeai_benchmark.sessions.onnxrt_session.ONNXRTSession'>\n",
      "{'advanced_options:quantization_scale_type': 4, 'tensor_bits': 8, 'accuracy_level': 1, 'debug_level': 0, 'inference_mode': 0, 'advanced_options:high_resolution_optimization': 0, 'advanced_options:pre_batchnorm_fold': 1, 'advanced_options:calibration_frames': 10, 'advanced_options:calibration_iterations': 10, 'advanced_options:activation_clipping': 1, 'advanced_options:weight_clipping': 1, 'advanced_options:bias_calibration': 1, 'advanced_options:output_feature_16bit_names_list': '', 'advanced_options:params_16bit_names_list': '', 'advanced_options:add_data_convert_ops': 3, 'advanced_options:c7x_firmware_version': '10_01_04_00', 'ti_internal_nc_flag': 83886080}\n"
     ]
    }
   ],
   "source": [
    "# choose one session_name depending on the model type\n",
    "# tflitert for tflite models, onnxrt for onnx model\n",
    "#session_name = constants.SESSION_NAME_TFLITERT\n",
    "session_name = constants.SESSION_NAME_ONNXRT\n",
    "\n",
    "session_type = settings.get_session_type(session_name)\n",
    "runtime_options = settings.get_runtime_options(session_name, is_qat=False)\n",
    "\n",
    "print(session_type)\n",
    "print(runtime_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b774e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_transforms = preprocess.PreProcessTransforms(settings)\n",
    "postproc_transforms = postprocess.PostProcessTransforms(settings)\n",
    "\n",
    "# these session cfgs also has some default input mean and scale. \n",
    "# if your model needs a difference mean and scale, update the session cfg dict being used with those values\n",
    "onnx_session_cfg = sessions.get_onnx_session_cfg(settings, work_dir=work_dir)\n",
    "tflite_session_cfg = sessions.get_tflite_session_cfg(settings, work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d9087",
   "metadata": {},
   "source": [
    "#### Create pipeline_configs\n",
    "\n",
    "pipeline_configs is nothing but a dict with the various model configs that we want to compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81db0343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cl-mnv2': {'task_type': 'classification', 'calibration_dataset': <edgeai_benchmark.datasets.image_cls.ImageClassification object at 0x79dd83dc34c0>, 'input_dataset': <edgeai_benchmark.datasets.image_cls.ImageClassification object at 0x79dd83dc3550>, 'preprocess': <edgeai_benchmark.preprocess.PreProcessTransforms object at 0x79dd83dc3ca0>, 'session': <edgeai_benchmark.sessions.onnxrt_session.ONNXRTSession object at 0x79dd83dc3c70>, 'postprocess': <edgeai_benchmark.postprocess.PostProcessTransforms object at 0x79dd83dc32b0>, 'model_info': {'metric_reference': {'accuracy_top1%': 71.88}}}}\n"
     ]
    }
   ],
   "source": [
    "pipeline_configs = {\n",
    "    'cl-mnv2': dict(\n",
    "        task_type='classification',\n",
    "        calibration_dataset=calib_dataset,\n",
    "        input_dataset=val_dataset,\n",
    "        preprocess=preproc_transforms.get_transform_onnx(),\n",
    "        session=session_type(**onnx_session_cfg,\n",
    "            runtime_options=runtime_options,\n",
    "            model_path=f'{settings.models_path}/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx'),\n",
    "        postprocess=postproc_transforms.get_transform_classification(),\n",
    "        model_info=dict(metric_reference={'accuracy_top1%':71.88})\n",
    "    ),\n",
    "}\n",
    "print(pipeline_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71fc46",
   "metadata": {},
   "source": [
    "#### Model Compilation\n",
    "\n",
    "This will take a few minutes. Please be patient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c124a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: number of configs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TASKS TOTAL=1, NUM_RUNNING=0:   0%|                                                                                                                                                                                                                                | 0/1 [00:00<?, ?it/s, postfix={'RUNNING': [], 'COMPLETED': []}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142507: starting - \u001b[39mcl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx\n",
      "\u001b[34mINFO:\u001b[33m20250226-142507: model_path - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "\u001b[34mINFO:\u001b[33m20250226-142507: model_file - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/model/mobilenet_v2_tv.onnx\n",
      "\u001b[34mINFO:\u001b[33m20250226-142507: quant_file - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/model/mobilenet_v2_tv_qparams.prototxt\n",
      "Downloading 1/1: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "Download done for /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "Downloading 1/1: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "Download done for /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TASKS TOTAL=1, NUM_RUNNING=1:   0%|                                                                                                                                                                                                                | 0/1 [00:00<?, ?it/s, postfix={'RUNNING': ['cl-mnv2:import'], 'COMPLETED': []}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted model is valid!\n",
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142507: running - \u001b[39mcl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx\n",
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142507: pipeline_config - \u001b[39m{'task_type': 'classification', 'calibration_dataset': <edgeai_benchmark.datasets.image_cls.ImageClassification object at 0x79dd83dc34c0>, 'input_dataset': <edgeai_benchmark.datasets.image_cls.ImageClassification object at 0x79dd83dc3550>, 'preprocess': <edgeai_benchmark.preprocess.PreProcessTransforms object at 0x79dd83dc3ca0>, 'session': <edgeai_benchmark.sessions.onnxrt_session.ONNXRTSession object at 0x79dd83dc3c70>, 'postprocess': <edgeai_benchmark.postprocess.PostProcessTransforms object at 0x79dd83dc32b0>, 'model_info': {'metric_reference': {'accuracy_top1%': 71.88}}}\n",
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142507: import  - \u001b[39mcl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx - this may take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):|                                                                                                                                                                                                                | 0/1 [00:03<?, ?it/s, postfix={'RUNNING': ['cl-mnv2:import'], 'COMPLETED': []}]\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/pipeline_runner.py\", line 264, in _run_pipeline\n",
      "    result = cls._run_pipeline_impl(settings, pipeline_config, description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/pipeline_runner.py\", line 240, in _run_pipeline_impl\n",
      "    accuracy_result = accuracy_pipeline(description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/accuracy_pipeline.py\", line 82, in __call__\n",
      "    param_result = self._run(description=description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/accuracy_pipeline.py\", line 107, in _run\n",
      "    self._import_model(description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/accuracy_pipeline.py\", line 171, in _import_model\n",
      "    session.import_model(calib_data)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/sessions/onnxrt_session.py\", line 69, in import_model\n",
      "    outputs = self.interpreter.run(output_keys, calib_dict)\n",
      "  File \"/home/a0393608local/.pyenv/versions/3.10.14/envs/benchmark/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 217, in run\n",
      "    return self._sess.run(output_names, input_feed, run_options)\n",
      "onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running TIDL_0 node. Name:'TIDLExecutionProvider_TIDL_0_0' Status Message: TIDL Compute Invoke Failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running TIDL_0 node. Name:'TIDLExecutionProvider_TIDL_0_0' Status Message: TIDL Compute Invoke Failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TASKS TOTAL=1, NUM_RUNNING=0:   0%|                                                                                                                                                                                                                                | 0/1 [00:04<?, ?it/s, postfix={'RUNNING': [], 'COMPLETED': []}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142511: starting - \u001b[39mcl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx\n",
      "\u001b[34mINFO:\u001b[33m20250226-142511: model_path - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "\u001b[34mINFO:\u001b[33m20250226-142511: model_file - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/model/mobilenet_v2_tv.onnx\n",
      "\u001b[34mINFO:\u001b[33m20250226-142511: quant_file - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/model/mobilenet_v2_tv_qparams.prototxt\n",
      "Downloading 1/1: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "Download done for /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "Downloading 1/1: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n",
      "Download done for /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-modelzoo/models/vision/classification/imagenet1k/torchvision/mobilenet_v2_tv.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TASKS TOTAL=1, NUM_RUNNING=1:   0%|                                                                                                                                                                                                                 | 0/1 [00:04<?, ?it/s, postfix={'RUNNING': ['cl-mnv2:infer'], 'COMPLETED': []}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted model is valid!\n",
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142511: running - \u001b[39mcl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx\n",
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142511: pipeline_config - \u001b[39m{'task_type': 'classification', 'calibration_dataset': <edgeai_benchmark.datasets.image_cls.ImageClassification object at 0x79dd83dc34c0>, 'input_dataset': <edgeai_benchmark.datasets.image_cls.ImageClassification object at 0x79dd83dc3550>, 'preprocess': <edgeai_benchmark.preprocess.PreProcessTransforms object at 0x79dd83dc3ca0>, 'session': <edgeai_benchmark.sessions.onnxrt_session.ONNXRTSession object at 0x79dd83dc3c70>, 'postprocess': <edgeai_benchmark.postprocess.PostProcessTransforms object at 0x79dd83dc32b0>, 'model_info': {'metric_reference': {'accuracy_top1%': 71.88}}}\n",
      "\u001b[34m\n",
      "INFO:\u001b[33m20250226-142511: infer  - \u001b[39mcl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx - this may take some time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/pipeline_runner.py\", line 264, in _run_pipeline\n",
      "    result = cls._run_pipeline_impl(settings, pipeline_config, description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/pipeline_runner.py\", line 240, in _run_pipeline_impl\n",
      "    accuracy_result = accuracy_pipeline(description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/accuracy_pipeline.py\", line 82, in __call__\n",
      "    param_result = self._run(description=description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/accuracy_pipeline.py\", line 131, in _run\n",
      "    output_list = self._infer_frames(description)\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/pipelines/accuracy_pipeline.py\", line 185, in _infer_frames\n",
      "    is_ok = session.start_infer()\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/sessions/onnxrt_session.py\", line 77, in start_infer\n",
      "    super().start_infer()\n",
      "  File \"/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/edgeai_benchmark/sessions/basert_session.py\", line 176, in start_infer\n",
      "    raise FileNotFoundError(error_message)\n",
      "FileNotFoundError: \u001b[35mERROR:\u001b[33m20250226-142511: artifacts_folder is missing, please run import (on pc) - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/artifacts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mERROR:\u001b[33m20250226-142511: artifacts_folder is missing, please run import (on pc) - \u001b[39m/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TASKS TOTAL=1, NUM_RUNNING=0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.43s/it, postfix={'RUNNING': [], 'COMPLETED': ['cl-mnv2']}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: compiled artifacts is in: /data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/result.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mINFO: compiled artifacts is in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print the result:\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m      8\u001b[0m     result_yaml \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fp)\n\u001b[1;32m      9\u001b[0m     result_dict \u001b[38;5;241m=\u001b[39m result_yaml[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/benchmark/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/ssd/files/a0393608/work/code/ti/edgeai-algo/edgeai-benchmark/work_dirs_custom/modelartifacts/8bits/cl-mnv2_onnxrt_imagenet1k_torchvision_mobilenet_v2_tv_onnx/result.yaml'"
     ]
    }
   ],
   "source": [
    "# run the model compliation/import and inference\n",
    "interfaces.run_benchmark_config(settings, work_dir, pipeline_configs)\n",
    "run_dir = list(pipeline_configs.values())[0]['session'].get_run_dir()\n",
    "print(f\"\\nINFO: compiled artifacts is in: {run_dir}\")\n",
    "\n",
    "# print the result:\n",
    "with open(os.path.join(run_dir, \"result.yaml\")) as fp:\n",
    "    result_yaml = yaml.safe_load(fp)\n",
    "    result_dict = result_yaml['result']\n",
    "    print(f\"INFO: result - {result_dict}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d982aa0",
   "metadata": {},
   "source": [
    "#### Package artifacts\n",
    "\n",
    "Package the artifacts into a .tar.gz file, keeping only the necessary files for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a664e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "out_dir = f'{work_dir}_package'\n",
    "interfaces.package_artifacts(settings, work_dir, out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01cc894",
   "metadata": {},
   "source": [
    "#### Download\n",
    "\n",
    "Download the packaged .tar.gz artifact\n",
    "\n",
    "TODO: add a download link here, that the user can click to download the packaged artifact .tar.gz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nINFO: download the atricats files from the folder: {out_dir}')\n",
    "print(os.listdir(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c237d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
