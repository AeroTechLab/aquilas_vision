# Usage

Compiling a DNN model is the process of quantizing and converting the model into a format that can be inferred with TIDL. TIDL (and its open source front ends) provides utilities to compile models. The imported artifacts can then be used to run inference.

In addition to what is provided with TIDL, this repository provides igh level utilities for compiling DNN models. These tools include dataset loaders, pre-processing utilities, post-processing utilities and metric computation utilities.

## Running inference / benchmark using pre-compiled models
[run_benchmarks_pc.sh](../run_benchmarks_pc.sh) is the main script in this repository that does compilation of models and benchmark. 

For the models in the model zoo, we have provided (link to) pre-compiled artifacts in the folder [work_dirs/modelartifacts/8bits](../work_dirs/modelartifacts/8bits). 

If these links are present, compilation will be skipped, those model artifacts will be downloaded automatically and inference/benchmark will be performed.

[run_benchmarks_j7.sh](../run_benchmarks_j7.sh) can be used to run these model artifacts on device. 


## Compile models in the model zoo

[run_benchmarks_pc.sh](../run_benchmarks_pc.sh) is the main script in this repository that does compilation of models and benchmark. 
- While running this script, compilation of model in the model zoo will be performed as the first step before the inference. 
- The imported artifacts can be used to run inference on the target device (eg. EVM). 

However, there are a few things to note:
- Model compilation, inference and benchmark can be run on PC (Simulation).
- However, the import step cannot be run on the target device (eg. EVM). So that import step has to be run on PC and the resulting artifacts has to be coped to the target device to perform inference or accuracy benchmark.

For the models in the model zoo, we have provided (link to) pre-compiled artifacts in the folder [work_dirs/modelartifacts/8bits](../work_dirs/modelartifacts/8bits). 
- If this links are present, comilation will be skipped, those model artifacts will be downloaded automatically and infernce/benchmark will be performed.
- If you like to do the compilation of these models yourself, you can either rename the folder [work_dirs/modelartifacts](../work_dirs/modelartifacts) or change the value of *modelartifacts_path* in [settings_import_on_pc.yaml](../settings_import_on_pc.yaml). 

If you have compiled models yourself and would like to run those model artifacts in the target device, run the script [run_package_artifacts_j7.sh](../run_package_artifacts_j7.sh) to package the artifacts for use in the target device.


## Generate report
A CSV report containing all your benchmarking results is generated by running [scripts/generate_report.py](../scripts/generate_report.py)

